{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.base import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Human Feedback ì‚¬ìš©í•˜ê¸°\n",
    "\n",
    "Interrupt ê±¸ì–´ì„œ Feedback ë°›ëŠ” ë°©ì‹ 2ê°€ì§€ ìˆìŒ.\n",
    "\n",
    "1. compile í• ë•Œ `interrupt_before` ì§€ì •\n",
    "\n",
    "```python \n",
    "graph = builder.compile(interrupt_before=[\"tools\"],\n",
    "                        checkpointer=ShortTermMemory)\n",
    "```\n",
    "\n",
    "\n",
    "2. ë…¸ë“œ ë‚´ë¶€ì—ì„œ `NodeInterrupt` ë°œìƒ\n",
    "\n",
    "```python\n",
    "raise NodeInterrupt(f\"ì…ë ¥ë°›ì€ ë¬¸ìì—´ì˜ ê¸¸ì´ê°€ ë„ˆë¬´ ê¹ë‹ˆë‹¤. ìµœëŒ€ 10ìê¹Œì§€ ì…ë ¥í•´ì£¼ì„¸ìš”.\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. ì²«ë²ˆì§¸ ë°©ì‹ (interrupt_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@trace_function()\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply a and b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "@trace_function()\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Adds a and b.\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "@trace_function()\n",
    "def divide(a: int, b: int) -> float:\n",
    "    \"\"\"Adds a and b.\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a / b\n",
    "\n",
    "tools = [add, multiply, divide]\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "@trace_function()\n",
    "def node_assistant(state: MessagesState):\n",
    "    system_message = SystemMessage(content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\")\n",
    "    return {\"messages\": [llm_with_tools.invoke([system_message] + state[\"messages\"])]}\n",
    "    \n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"node_assistant\", node_assistant)\n",
    "builder.add_node(\"tools\", ToolNode(tools))\n",
    "builder.add_edge(START, \"node_assistant\")\n",
    "builder.add_conditional_edges(\"node_assistant\",tools_condition)\n",
    "builder.add_edge(\"tools\", \"node_assistant\")\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "breakpoint ê°„ë‹¨ ì˜ˆì œ - Tool Node ì— Breakpoint ì„¤ì •\n",
    "\n",
    "- í•¨ìˆ˜ë“¤ì—ë„ `trace_function` ë°ì½”ë ˆì´í„°ë¥¼ ë¶™ì˜€ì§€ë§Œ, í†µê³¼í•˜ì§€ ì•ŠìŒ.\n",
    "\n",
    "- stateì˜ messages ê°’ì˜ ë§ˆì§€ë§‰ë„ tool call ìš”ì²­ë¬¸\n",
    "\n",
    "- Graph ì´ì–´ì„œ ì‹¤í–‰ì‹œ, ì‚¬ìš©ìì—ê²Œ ì…ë ¥ë°›ìŒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ShortTermMemory = MemorySaver()\n",
    "graph_with_interrupt_tools = builder.compile(interrupt_before=[\"tools\"], # tools ë…¸ë“œ ì „ì— ë©ˆì¶¤\n",
    "                                             checkpointer=ShortTermMemory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m\n",
      "ğŸš€ Passing Through [node_assistant] ..\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Multiply 2 and 3', additional_kwargs={}, response_metadata={}, id='1b6cdd68-2c47-4e0e-a6be-3a460da3cf91'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_ZayrQgsWoY1RfNaN3EMVjNGR', 'function': {'arguments': '{\"a\":2,\"b\":3}', 'name': 'multiply'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 143, 'total_tokens': 161, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_703d4ff298', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-4ab5566f-7026-4814-80c9-7dabecdc52d1-0', tool_calls=[{'name': 'multiply', 'args': {'a': 2, 'b': 3}, 'id': 'call_ZayrQgsWoY1RfNaN3EMVjNGR', 'type': 'tool_call'}], usage_metadata={'input_tokens': 143, 'output_tokens': 18, 'total_tokens': 161, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_with_interrupt_tools.invoke({\"messages\": \"Multiply 2 and 3\"}, config)\n",
    "graph_with_interrupt_tools.get_state(config).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m\n",
      "ğŸš€ Passing Through [multiply] ..\u001b[0m\n",
      "\u001b[92m\n",
      "ğŸš€ Passing Through [node_assistant] ..\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "user_approval = input(\"Do you want to call the tool? (yes/no): \")\n",
    "if user_approval.lower() == \"yes\":\n",
    "    graph_with_interrupt_tools.invoke(None, config) # yes ì¼ ê²½ìš° tool ë…¸ë“œ í˜¸ì¶œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Multiply 2 and 3', additional_kwargs={}, response_metadata={}, id='1b6cdd68-2c47-4e0e-a6be-3a460da3cf91'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_ZayrQgsWoY1RfNaN3EMVjNGR', 'function': {'arguments': '{\"a\":2,\"b\":3}', 'name': 'multiply'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 143, 'total_tokens': 161, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_703d4ff298', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-4ab5566f-7026-4814-80c9-7dabecdc52d1-0', tool_calls=[{'name': 'multiply', 'args': {'a': 2, 'b': 3}, 'id': 'call_ZayrQgsWoY1RfNaN3EMVjNGR', 'type': 'tool_call'}], usage_metadata={'input_tokens': 143, 'output_tokens': 18, 'total_tokens': 161, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       "  ToolMessage(content='6', name='multiply', id='42d5d819-f432-4ca0-90e3-6a51eb57e977', tool_call_id='call_ZayrQgsWoY1RfNaN3EMVjNGR'),\n",
       "  AIMessage(content='The result of multiplying 2 and 3 is 6.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 168, 'total_tokens': 183, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_703d4ff298', 'finish_reason': 'stop', 'logprobs': None}, id='run-79a79855-8694-4f18-9c62-eb8025f474e9-0', usage_metadata={'input_tokens': 168, 'output_tokens': 15, 'total_tokens': 183, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_with_interrupt_tools.get_state(config).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Humanfeedback ê°„ë‹¨ ì˜ˆì œ - Assistant Node ì— Breakpoint ì„¤ì •\n",
    "\n",
    "- stateì˜ messages ê°’ì˜ ë§ˆì§€ë§‰ì´ ì‚¬ìš©ì ìš”ì²­ë¬¸\n",
    "\n",
    "- `update_state`ë¥¼ í†µí•´ ì‚¬ìš©ì ìš”ì²­ë¬¸ ì¬ì…ë ¥ ê°€ëŠ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ShortTermMemory = MemorySaver()\n",
    "graph_with_interrupt_assitant = builder.compile(interrupt_before=[\"node_assistant\"], # node_assistant ë…¸ë“œ ì „ì— ë©ˆì¶¤\n",
    "                                                checkpointer=ShortTermMemory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='2 ê³±í•˜ê¸° 3ì€?', additional_kwargs={}, response_metadata={}, id='ae8a28e0-aa9c-4b74-b277-73042809da34')]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_with_interrupt_assitant.invoke({\"messages\": \"2 ê³±í•˜ê¸° 3ì€?\"}, config)\n",
    "graph_with_interrupt_assitant.get_state(config).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m\n",
      "ğŸš€ Passing Through [node_assistant] ..\u001b[0m\n",
      "\u001b[92m\n",
      "ğŸš€ Passing Through [multiply] ..\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='2 ê³±í•˜ê¸° 3ì€?', additional_kwargs={}, response_metadata={}, id='ae8a28e0-aa9c-4b74-b277-73042809da34'),\n",
       "  HumanMessage(content='ì•„! ì˜ëª»ë§í—€ë‹¤. 3 ê³±í•˜ê¸° 3ì€?', additional_kwargs={}, response_metadata={}, id='93a5f926-7528-4c1a-b491-a082977a25db'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_igx5NQrC4S6yhFWlYBBxdO5Q', 'function': {'arguments': '{\"a\":3,\"b\":3}', 'name': 'multiply'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 167, 'total_tokens': 185, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_703d4ff298', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-14ed5433-d7e6-4b72-950a-81402ccd6d41-0', tool_calls=[{'name': 'multiply', 'args': {'a': 3, 'b': 3}, 'id': 'call_igx5NQrC4S6yhFWlYBBxdO5Q', 'type': 'tool_call'}], usage_metadata={'input_tokens': 167, 'output_tokens': 18, 'total_tokens': 185, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       "  ToolMessage(content='9', name='multiply', id='91b8d520-bb1f-4c01-b36d-12663b6a6f7d', tool_call_id='call_igx5NQrC4S6yhFWlYBBxdO5Q')]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_with_interrupt_assitant.update_state(config,\n",
    "                                           {\"messages\": [HumanMessage(content=\"ì•„! ì˜ëª»ë§í—€ë‹¤. 3 ê³±í•˜ê¸° 3ì€?\")]})\n",
    "graph_with_interrupt_assitant.invoke(None, config) # ì‹¤ì œë¡œ 3 x 3 ê³„ì‚° ìˆ˜í–‰\n",
    "graph_with_interrupt_assitant.get_state(config).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Node ë‚´ë¶€ì—ì„œ NodeInterrupt ë°œìƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.errors import NodeInterrupt\n",
    "\n",
    "@trace_function()\n",
    "def step_1(state:MessagesState) -> MessagesState:\n",
    "    return state\n",
    "\n",
    "@trace_function()\n",
    "def step_2(state:MessagesState) -> MessagesState:\n",
    "    if len(state['messages'][-1].content) > 5:\n",
    "        print(f\"Received input that is longer than 5 characters: {state['messages'][-1].content}\")\n",
    "        raise NodeInterrupt(f\"Received input that is longer than 5 characters: {state['messages'][-1].content}\")\n",
    "    return state\n",
    "\n",
    "@trace_function()\n",
    "def step_3(state:MessagesState) -> MessagesState:\n",
    "    return state\n",
    "\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"step_1\", step_1)\n",
    "builder.add_node(\"step_2\", step_2)\n",
    "builder.add_node(\"step_3\", step_3)\n",
    "builder.add_edge(START, \"step_1\")\n",
    "builder.add_edge(\"step_1\", \"step_2\")\n",
    "builder.add_edge(\"step_2\", \"step_3\")\n",
    "builder.add_edge(\"step_3\", END)\n",
    "memory = MemorySaver()\n",
    "graph_with_node_interrupt = builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m\n",
      "ğŸš€ Passing Through [step_1] ..\u001b[0m\n",
      "\u001b[92m\n",
      "ğŸš€ Passing Through [step_2] ..\u001b[0m\n",
      "Received input that is longer than 5 characters: ì•ˆë…•í•˜ì„¸ìš”, ì €ëŠ” ì°½ìš°ë¼ê³  í•©ë‹ˆë‹¤.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='ì•ˆë…•í•˜ì„¸ìš”, ì €ëŠ” ì°½ìš°ë¼ê³  í•©ë‹ˆë‹¤.', additional_kwargs={}, response_metadata={}, id='aafc4bde-9582-4c4d-ad2f-aab4ba665ef6')]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_with_node_interrupt.invoke({\"messages\": \"ì•ˆë…•í•˜ì„¸ìš”, ì €ëŠ” ì°½ìš°ë¼ê³  í•©ë‹ˆë‹¤.\"}, config)\n",
    "graph_with_node_interrupt.get_state(config).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m\n",
      "ğŸš€ Passing Through [step_2] ..\u001b[0m\n",
      "\u001b[92m\n",
      "ğŸš€ Passing Through [step_3] ..\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='ì•ˆë…•í•˜ì„¸ìš”, ì €ëŠ” ì°½ìš°ë¼ê³  í•©ë‹ˆë‹¤.', additional_kwargs={}, response_metadata={}, id='aafc4bde-9582-4c4d-ad2f-aab4ba665ef6'),\n",
       "  HumanMessage(content='ì•ˆë…•', additional_kwargs={}, response_metadata={}, id='0e40a3e4-66a4-4427-a48a-d2246dd74a3b')]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_with_node_interrupt.update_state(config, {\"messages\": [HumanMessage(content=\"ì•ˆë…•\")]})\n",
    "graph_with_node_interrupt.invoke(None, config)\n",
    "graph_with_node_interrupt.get_state(config).values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
